{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be862555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#!{sys.executable} -m pip install pandas\n",
    "#!{sys.executable} -m pip install numpy\n",
    "#!{sys.executable} -m pip install sklearn\n",
    "#!{sys.executable} -m pip install seaborn\n",
    "#!{sys.executable} -m pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d67730b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f12e553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a1fb0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27cede41",
   "metadata": {},
   "outputs": [],
   "source": [
    "## count the number of characters\n",
    "def count_chars(text):\n",
    "    return len(text)\n",
    "\n",
    "## count the number of words\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "## count the number of sentences\n",
    "def count_sent(text):\n",
    "    return len(nltk.sent_tokenize(text))\n",
    "\n",
    "## count the number of unique words within the tweet\n",
    "def count_unique(text):\n",
    "    return len(set(text.split()))\n",
    "\n",
    "## count hashtags\n",
    "def count_htags(text):\n",
    "    return len(re.findall(r'(#w[A-Za-z0-9]*)', text))\n",
    "\n",
    "## count capital letters\n",
    "def count_capital_chars(text):\n",
    "    count = 0\n",
    "    for x in text:\n",
    "        if x.isupper():\n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "## count capital words\n",
    "def count_capital_words(text):\n",
    "    return sum(map(str.isupper, text.split()))\n",
    "\n",
    "## count stopwords\n",
    "def count_stopwords(text):\n",
    "    ## not sure how to do\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "239f823e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\data\\sample_submission.csv\n",
      ".\\data\\test.csv\n",
      ".\\data\\train.csv\n",
      "load data done\n",
      "training: (7613, 5)\n",
      "total keywords NA: 221\n",
      "total locations NA:3341\n"
     ]
    }
   ],
   "source": [
    "    for dirname, _, filenames in os.walk('.\\data'):\n",
    "        for filename in filenames:\n",
    "            print(os.path.join(dirname, filename))\n",
    "\n",
    "    train = pd.read_csv('./data/train.csv')\n",
    "    print(\"load data done\")\n",
    "    \n",
    "    print('training: ' + str(train.shape))\n",
    "    print('total keywords NA: ' + str(train.keyword.nunique()))\n",
    "    print('total locations NA:' + str(train.location.nunique()))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8db3757f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  target  char_count  \\\n",
      "0  Our Deeds are the Reason of this #earthquake M...       1          69   \n",
      "1             Forest fire near La Ronge Sask. Canada       1          38   \n",
      "2  All residents asked to 'shelter in place' are ...       1         133   \n",
      "3  13,000 people receive #wildfires evacuation or...       1          65   \n",
      "4  Just got sent this photo from Ruby #Alaska as ...       1          88   \n",
      "\n",
      "   word_count  sent_count  cap_char_count  cap_word_count  unique_word_count  \\\n",
      "0          13           1              10               1                 13   \n",
      "1           7           2               5               0                  7   \n",
      "2          22           2               2               0                 20   \n",
      "3           8           1               1               0                  8   \n",
      "4          16           1               3               0                 15   \n",
      "\n",
      "   htag_count  avg_word_length  avg_sentence_length  unique_v_words  \n",
      "0           0         5.307692                 13.0        1.000000  \n",
      "1           0         5.428571                  3.5        1.000000  \n",
      "2           0         6.045455                 11.0        0.909091  \n",
      "3           1         8.125000                  8.0        1.000000  \n",
      "4           1         5.500000                 16.0        0.937500  \n"
     ]
    }
   ],
   "source": [
    "    # make feature columns\n",
    "    train['char_count'] = train['text'].apply(lambda x:count_chars(x))\n",
    "    train['word_count'] = train['text'].apply(lambda x:count_words(x))\n",
    "    train['sent_count'] = train['text'].apply(lambda x:count_sent(x))\n",
    "    train['cap_char_count'] = train['text'].apply(lambda x:count_capital_chars(x))\n",
    "    train['cap_word_count'] = train['text'].apply(lambda x:count_capital_words(x))\n",
    "    train['unique_word_count'] = train['text'].apply(lambda x:count_unique(x))\n",
    "    train['htag_count'] = train['text'].apply(lambda x:count_htags(x))\n",
    "    # average word length\n",
    "    train['avg_word_length'] = train['char_count']/train['word_count']\n",
    "    # average sentence length\n",
    "    train['avg_sentence_length'] = train['word_count']/train['sent_count']\n",
    "    # fraction of unique to total words\n",
    "    train['unique_v_words'] = train['unique_word_count']/train['word_count']\n",
    "    \n",
    "    train.drop(columns=['id', 'keyword', 'location'],inplace=True)\n",
    "    print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d781d3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  target  char_count  \\\n",
      "0  our deeds are the reason of this earthquake ma...       1          69   \n",
      "1              forest fire near la ronge sask canada       1          38   \n",
      "2  all residents asked to shelter in place are be...       1         133   \n",
      "3    people receive wildfires evacuation orders i...       1          65   \n",
      "4  just got sent this photo from ruby alaska as s...       1          88   \n",
      "\n",
      "   word_count  sent_count  cap_char_count  cap_word_count  unique_word_count  \\\n",
      "0          13           1              10               1                 13   \n",
      "1           7           2               5               0                  7   \n",
      "2          22           2               2               0                 20   \n",
      "3           8           1               1               0                  8   \n",
      "4          16           1               3               0                 15   \n",
      "\n",
      "   htag_count  avg_word_length  avg_sentence_length  unique_v_words  \n",
      "0           0         5.307692                 13.0        1.000000  \n",
      "1           0         5.428571                  3.5        1.000000  \n",
      "2           0         6.045455                 11.0        0.909091  \n",
      "3           1         8.125000                  8.0        1.000000  \n",
      "4           1         5.500000                 16.0        0.937500  \n"
     ]
    }
   ],
   "source": [
    "def remove_links(tweet):\n",
    "    '''Takes a string and removes web links from it'''\n",
    "    tweet = re.sub(r'httpS+', '', tweet) # remove http links\n",
    "    tweet = re.sub(r'bit.ly/S+', '', tweet) # rempve bitly links\n",
    "    tweet = tweet.strip('[link]') # remove [links]\n",
    "    return tweet\n",
    "def remove_users(tweet):\n",
    "    '''Takes a string and removes retweet and @user information'''\n",
    "    tweet = re.sub('(RTs@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove retweet\n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove tweeted at\n",
    "    return tweet\n",
    "def preprocess(sent):\n",
    "    sent = remove_users(sent)\n",
    "    sent = remove_links(sent)\n",
    "    sent = sent.lower() # lower case\n",
    "    sent = re.sub('['+string.punctuation + ']+', ' ', sent) # strip punctuation\n",
    "    sent = re.sub(r'\\s+',' ', sent) #remove double spacing\n",
    "    sent = re.sub('([0-9]+)', '', sent) # remove numbers\n",
    "    sent_token_list = [word for word in sent.split(' ')]\n",
    "    sent = ' '.join(sent_token_list)\n",
    "    return(sent)\n",
    "train['text'] = train['text'].apply(lambda x: preprocess(x))\n",
    "print(train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a6e88bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtainTargets(data):\n",
    "    return data['target'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf38d1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train['text'] = train['text'].apply(lambda x: remove_URL(x)) \\\n",
    " #             .apply(lambda x: remove_html(x)) \\\n",
    " #              .apply(lambda x: remove_emoji(x)) \\\n",
    " #             .apply(lambda x: remove_at(x)) \\\n",
    "#.apply(lambda x: remove_punct(x)) \n",
    "vectorizer = TfidfVectorizer()\n",
    "train_tf_idf_features =  vectorizer.fit_transform(train['text']).toarray()\n",
    "\n",
    "train_tf_idf = pd.DataFrame(train_tf_idf_features)\n",
    "\n",
    "train_Y = obtainTargets(train)\n",
    "train.drop(columns=['target'])\n",
    "features = ['char_count', 'word_count', 'sent_count',\n",
    "       'cap_char_count', 'cap_word_count', 'unique_word_count', 'htag_count',\n",
    "        'avg_word_length', 'avg_sentence_length', 'unique_v_words']\n",
    "\n",
    "train = pd.merge(train_tf_idf,train[features],left_index=True, right_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f70bb7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 18863)\n",
      "<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(type(train[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20207941",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train, train_Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc49536",
   "metadata": {},
   "outputs": [],
   "source": [
    "_RandomForestClassifier = RandomForestClassifier(n_estimators = 1000, min_samples_split = 15, random_state = 42)\n",
    "_RandomForestClassifier.fit(X_train, y_train)\n",
    "_RandomForestClassifier_prediction = _RandomForestClassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7204311",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 4478 epochs took 3852 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed: 64.2min finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "logistic_regression_model = LogisticRegression(multi_class='ovr', verbose=3, solver=\"saga\", max_iter=10000, n_jobs = -1).fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58d14b39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =>  77.54\n"
     ]
    }
   ],
   "source": [
    "lr_prediction = logistic_regression_model.predict(X_test)\n",
    "print(\"Accuracy => \", round(accuracy_score(lr_prediction, y_test)*100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3025c10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
