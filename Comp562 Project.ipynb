{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "be862555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#!{sys.executable} -m pip install pandas\n",
    "#!{sys.executable} -m pip install numpy\n",
    "#!{sys.executable} -m pip install sklearn\n",
    "#!{sys.executable} -m pip install seaborn\n",
    "#!{sys.executable} -m pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d67730b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1f12e553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "27cede41",
   "metadata": {},
   "outputs": [],
   "source": [
    "## count the number of characters\n",
    "def count_chars(text):\n",
    "    return len(text)\n",
    "\n",
    "## count the number of words\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "## count the number of sentences\n",
    "def count_sent(text):\n",
    "    return len(nltk.sent_tokenize(text))\n",
    "\n",
    "## count the number of unique words within the tweet\n",
    "def count_unique(text):\n",
    "    return len(set(text.split()))\n",
    "\n",
    "## count hashtags\n",
    "def count_htags(text):\n",
    "    return len(re.findall(r'(#w[A-Za-z0-9]*)', text))\n",
    "\n",
    "## count capital letters\n",
    "def count_capital_chars(text):\n",
    "    count = 0\n",
    "    for x in text:\n",
    "        if x.isupper():\n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "## count capital words\n",
    "def count_capital_words(text):\n",
    "    return sum(map(str.isupper, text.split()))\n",
    "\n",
    "## count stopwords\n",
    "def count_stopwords(text):\n",
    "    ## not sure how to do\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "239f823e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\data\\sample_submission.csv\n",
      ".\\data\\test.csv\n",
      ".\\data\\train.csv\n",
      "load data done\n",
      "training: (7613, 5)\n",
      "total keywords NA: 221\n",
      "total locations NA:3341\n"
     ]
    }
   ],
   "source": [
    "    for dirname, _, filenames in os.walk('.\\data'):\n",
    "        for filename in filenames:\n",
    "            print(os.path.join(dirname, filename))\n",
    "\n",
    "    train = pd.read_csv('./data/train.csv')\n",
    "    print(\"load data done\")\n",
    "    \n",
    "    print('training: ' + str(train.shape))\n",
    "    print('total keywords NA: ' + str(train.keyword.nunique()))\n",
    "    print('total locations NA:' + str(train.location.nunique()))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "8db3757f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  target  char_count  \\\n",
      "0  Our Deeds are the Reason of this #earthquake M...       1          69   \n",
      "1             Forest fire near La Ronge Sask. Canada       1          38   \n",
      "2  All residents asked to 'shelter in place' are ...       1         133   \n",
      "3  13,000 people receive #wildfires evacuation or...       1          65   \n",
      "4  Just got sent this photo from Ruby #Alaska as ...       1          88   \n",
      "\n",
      "   word_count  sent_count  cap_char_count  cap_word_count  unique_word_count  \\\n",
      "0          13           1              10               1                 13   \n",
      "1           7           2               5               0                  7   \n",
      "2          22           2               2               0                 20   \n",
      "3           8           1               1               0                  8   \n",
      "4          16           1               3               0                 15   \n",
      "\n",
      "   htag_count  avg_word_length  avg_sentence_length  unique_v_words  \n",
      "0           0         5.307692                 13.0        1.000000  \n",
      "1           0         5.428571                  3.5        1.000000  \n",
      "2           0         6.045455                 11.0        0.909091  \n",
      "3           1         8.125000                  8.0        1.000000  \n",
      "4           1         5.500000                 16.0        0.937500  \n"
     ]
    }
   ],
   "source": [
    "    # make feature columns\n",
    "    train['char_count'] = train['text'].apply(lambda x:count_chars(x))\n",
    "    train['word_count'] = train['text'].apply(lambda x:count_words(x))\n",
    "    train['sent_count'] = train['text'].apply(lambda x:count_sent(x))\n",
    "    train['cap_char_count'] = train['text'].apply(lambda x:count_capital_chars(x))\n",
    "    train['cap_word_count'] = train['text'].apply(lambda x:count_capital_words(x))\n",
    "    train['unique_word_count'] = train['text'].apply(lambda x:count_unique(x))\n",
    "    train['htag_count'] = train['text'].apply(lambda x:count_htags(x))\n",
    "    # average word length\n",
    "    train['avg_word_length'] = train['char_count']/train['word_count']\n",
    "    # average sentence length\n",
    "    train['avg_sentence_length'] = train['word_count']/train['sent_count']\n",
    "    # fraction of unique to total words\n",
    "    train['unique_v_words'] = train['unique_word_count']/train['word_count']\n",
    "    \n",
    "    train.drop(columns=['id', 'keyword', 'location'],inplace=True)\n",
    "    print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "d781d3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  target  char_count  \\\n",
      "0  our deeds are the reason of this earthquake ma...       1          69   \n",
      "1              forest fire near la ronge sask canada       1          38   \n",
      "2  all residents asked to shelter in place are be...       1         133   \n",
      "3    people receive wildfires evacuation orders i...       1          65   \n",
      "4  just got sent this photo from ruby alaska as s...       1          88   \n",
      "\n",
      "   word_count  sent_count  cap_char_count  cap_word_count  unique_word_count  \\\n",
      "0          13           1              10               1                 13   \n",
      "1           7           2               5               0                  7   \n",
      "2          22           2               2               0                 20   \n",
      "3           8           1               1               0                  8   \n",
      "4          16           1               3               0                 15   \n",
      "\n",
      "   htag_count  avg_word_length  avg_sentence_length  unique_v_words  \n",
      "0           0         5.307692                 13.0        1.000000  \n",
      "1           0         5.428571                  3.5        1.000000  \n",
      "2           0         6.045455                 11.0        0.909091  \n",
      "3           1         8.125000                  8.0        1.000000  \n",
      "4           1         5.500000                 16.0        0.937500  \n"
     ]
    }
   ],
   "source": [
    "def remove_links(tweet):\n",
    "    '''Takes a string and removes web links from it'''\n",
    "    tweet = re.sub(r'httpS+', '', tweet) # remove http links\n",
    "    tweet = re.sub(r'bit.ly/S+', '', tweet) # rempve bitly links\n",
    "    tweet = tweet.strip('[link]') # remove [links]\n",
    "    return tweet\n",
    "def remove_users(tweet):\n",
    "    '''Takes a string and removes retweet and @user information'''\n",
    "    tweet = re.sub('(RTs@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove retweet\n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove tweeted at\n",
    "    return tweet\n",
    "def preprocess(sent):\n",
    "    sent = remove_users(sent)\n",
    "    sent = remove_links(sent)\n",
    "    sent = sent.lower() # lower case\n",
    "    sent = re.sub('['+string.punctuation + ']+', ' ', sent) # strip punctuation\n",
    "    sent = re.sub(r'\\s+',' ', sent) #remove double spacing\n",
    "    sent = re.sub('([0-9]+)', '', sent) # remove numbers\n",
    "    sent_token_list = [word for word in sent.split(' ')]\n",
    "    sent = ' '.join(sent_token_list)\n",
    "    return(sent)\n",
    "train['text'] = train['text'].apply(lambda x: preprocess(x))\n",
    "print(train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6a6e88bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtainTargets(data):\n",
    "    return data['target'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "bf38d1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#train['text'] = train['text'].apply(lambda x: remove_URL(x)) \\\n",
    " #             .apply(lambda x: remove_html(x)) \\\n",
    " #              .apply(lambda x: remove_emoji(x)) \\\n",
    " #             .apply(lambda x: remove_at(x)) \\\n",
    "#.apply(lambda x: remove_punct(x)) \n",
    "vectorizer = TfidfVectorizer()\n",
    "train_tf_idf_features =  vectorizer.fit_transform(train['text'].astype(str).values.tolist()).toarray()\n",
    "print(train_tf_idf_features)\n",
    "train_tf_idf = pd.DataFrame(train_tf_idf_features)\n",
    "\n",
    "\n",
    "train_Y = obtainTargets(train)\n",
    "train.drop(columns=['target'])\n",
    "features = ['char_count', 'word_count', 'sent_count',\n",
    "       'cap_char_count', 'cap_word_count', 'unique_word_count', 'htag_count',\n",
    "        'avg_word_length', 'avg_sentence_length', 'unique_v_words']\n",
    "\n",
    "train = pd.merge(train_tf_idf,train[features],left_index=True, right_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "8dd2232f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1    2    3    4    5    6    7    8    9  ...  char_count  \\\n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...          69   \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...          38   \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...         133   \n",
      "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...          65   \n",
      "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...          88   \n",
      "\n",
      "   word_count  sent_count  cap_char_count  cap_word_count  unique_word_count  \\\n",
      "0          13           1              10               1                 13   \n",
      "1           7           2               5               0                  7   \n",
      "2          22           2               2               0                 20   \n",
      "3           8           1               1               0                  8   \n",
      "4          16           1               3               0                 15   \n",
      "\n",
      "   htag_count  avg_word_length  avg_sentence_length  unique_v_words  \n",
      "0           0         5.307692                 13.0        1.000000  \n",
      "1           0         5.428571                  3.5        1.000000  \n",
      "2           0         6.045455                 11.0        0.909091  \n",
      "3           1         8.125000                  8.0        1.000000  \n",
      "4           1         5.500000                 16.0        0.937500  \n",
      "\n",
      "[5 rows x 18863 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "20207941",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train, train_Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c7204311",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 92 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicho\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.5min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-160-fd4c23c2b9b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlogistic_regression_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"saga\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogistic_regression_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1872\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'The TF-IDF vectorizer is not fitted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1873\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1874\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1875\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1876\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1254\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1255\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1256\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1257\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1115\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1116\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1117\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \"\"\"\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "logistic_regression_model = LogisticRegression(verbose=3, solver=\"saga\").fit(X_train, y_train)\n",
    "print(logistic_regression_model.score(vectorizer.transform(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d14b39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3025c10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
